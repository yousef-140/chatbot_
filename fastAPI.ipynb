{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1d70b0-5769-42ad-8f83-5238ab6854f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "\n",
    "client = Together(api_key=\"\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are some fun things to do in New York?\"\n",
    "      }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff32c85-54da-4b75-b7c8-f2556634a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [9604]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-49' coro=<Server.serve() done, defined at C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\uvicorn\\server.py:69> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\uvicorn\\main.py\", line 580, in run\n",
      "    server.run()\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\uvicorn\\server.py\", line 67, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\aldawlia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\aldawlia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 360, in __wakeup\n",
      "    self.__step()\n",
      "  File \"C:\\Users\\aldawlia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\uvicorn\\server.py\", line 70, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"C:\\Users\\aldawlia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"C:\\Users\\aldawlia\\Desktop\\tf\\tfvenv\\Lib\\site-packages\\uvicorn\\server.py\", line 331, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63294 - \"POST /llm/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63297 - \"POST /llm/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [9604]\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# GET endpoint - Hello with name\n",
    "@app.get(\"/a/{name}\")\n",
    "def say_hello(name: str):\n",
    "    return {\"message\": f\"hello {name}\"}\n",
    "\n",
    "# Request model\n",
    "class Query_bot(BaseModel):\n",
    "    messages: list[dict]  \n",
    "\n",
    "# Response model\n",
    "class chat_r(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "@app.post(\"/llm/\")\n",
    "def l_l_m(query: Query_bot):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        messages=query.messages \n",
    "    )\n",
    "    model_answer = response.choices[0].message.content\n",
    "    return chat_r(answer=model_answer)\n",
    "\n",
    "# Run FastAPI\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5fcd1-5087-4d1b-b975-fc853a3c37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71e3c8-e1ae-4ddc-94bf-ee9c6ed9cdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
